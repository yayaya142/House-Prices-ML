{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House Prices - Advanced Regression Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****** HERE WILL BE ID *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TL;DR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Imports and Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy, matplotlib, etc. \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import sweetviz as sw\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn import metrics\n",
    "from sklearn import pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neural_network\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# define plt settings\n",
    "sns.set_theme()\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "plt.rcParams[\"axes.labelsize\"] = 20\n",
    "plt.rcParams[\"xtick.labelsize\"] = 20\n",
    "plt.rcParams[\"ytick.labelsize\"] = 20\n",
    "plt.rcParams[\"legend.fontsize\"] = 20\n",
    "plt.rcParams[\"legend.markerscale\"] = 1.5\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "plt.rcParams[\"legend.title_fontsize\"] = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define the input and output folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"input/\"\n",
    "\n",
    "train_data_path = os.path.join(input_folder, \"train.csv\")\n",
    "test_data_path = os.path.join(input_folder, \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - define the show graphs variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_GRAPHS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the traning data\n",
    "  - Load the csv data to variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_path)\n",
    "\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# display the first few rows of the data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Data Investigation EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove the id column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"Id\" column from the train_data DataFrame\n",
    "train_data = train_data.drop(\"Id\", axis=1)\n",
    "\n",
    "# Drop the \"Id\" column from the test_data DataFrame\n",
    "test_id = test_data[\"Id\"]\n",
    "test_data = test_data.drop(\"Id\", axis=1)\n",
    "\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count the number of feuatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of features: {train_data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get summary statistics for the training dataset show only the numerical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the data types of the columns in the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the data is object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_missing_data_with_percentage(data):\n",
    "    print(\"Missing values in the dataset:\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Total Rows: \", len(data))\n",
    "    print(\"_________________________________________\")\n",
    "    # Display missing values in each column of the training dataset\n",
    "    missing_values = data.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(train_data)) * 100\n",
    "    missing_data = pd.concat([missing_values, missing_percentage], axis=1, keys=['Missing Values', 'Percentage'])\n",
    "    missing_data.sort_values(by='Missing Values', ascending=False, inplace=True)\n",
    "    print(missing_data.head(20))\n",
    "    \n",
    "    print(\"\\n\\nTotal missing values: \", missing_data['Missing Values'].sum())\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display missing values in the training dataset\n",
    "show_missing_data_with_percentage(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the columns \"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\" have a lot of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handle the missing data<br><br>\n",
    "First step to remove highly missing features (by threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_highly_missing_features(data, fetures_to_drop):\n",
    "    data = data.drop(fetures_to_drop, axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def find_features_with_missing_values_threshold(data, threshold):\n",
    "    missing_values = data.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(train_data)) * 100\n",
    "    missing_data = pd.concat([missing_values, missing_percentage], axis=1, keys=['Missing Values', 'Percentage'])\n",
    "    missing_data.sort_values(by='Missing Values', ascending=False, inplace=True)\n",
    "    features_to_drop = missing_data[missing_data['Percentage'] > threshold].index\n",
    "    return features_to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for missing values to remove\n",
    "threshold = 80\n",
    "# for 80 it return # [\"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\"]\n",
    "drop_features = find_features_with_missing_values_threshold(train_data, threshold) \n",
    "\n",
    "\n",
    "train_data = drop_highly_missing_features(train_data, drop_features)\n",
    "\n",
    "test_data = drop_highly_missing_features(test_data, drop_features)\n",
    "\n",
    "print(\"Remove this features: \", drop_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_missing_data_with_percentage(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Missing Values for Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing numerical values with median\n",
    "def handle_missing_values_numerical(data):\n",
    "    for column in data.select_dtypes(include=[np.number]).columns:\n",
    "        data[column].fillna(data[column].median(), inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Missing Values for Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing categorical values with most frequent value\n",
    "def handle_missing_values_categorical(data):\n",
    "    for column in data.select_dtypes(include=[object]).columns:\n",
    "        data[column].fillna(data[column].mode()[0], inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One function to handle the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(data):\n",
    "    data = handle_missing_values_numerical(data)\n",
    "    data = handle_missing_values_categorical(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing values in the train data\n",
    "train_data = handle_missing_values(train_data)\n",
    "\n",
    "# fill the missing values in the test data\n",
    "test_data = handle_missing_values(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify No More Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values in the training dataset after filling:\")\n",
    "print(train_data.isnull().sum().sum())\n",
    "\n",
    "print(\"\\nMissing values in the test dataset after filling:\")\n",
    "print(test_data.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Categorical Features to Numeric Using One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data types in training data:\")\n",
    "print(train_data.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_columns = train_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Use one hot encoding to convert categorical columns to numerical columns\n",
    "train_data_encoded = pd.get_dummies(train_data, columns=categorical_columns)\n",
    "test_data_encoded = pd.get_dummies(test_data, columns=categorical_columns)\n",
    "\n",
    "# Save the target variable (SalePrice) and then drop it from train_data_encoded before alignment\n",
    "sale_price = train_data_encoded['SalePrice']\n",
    "train_data_encoded = train_data_encoded.drop('SalePrice', axis=1)\n",
    "\n",
    "# Align train and test data to ensure they have the same columns\n",
    "train_data_encoded, test_data_encoded = train_data_encoded.align(test_data_encoded, join='inner', axis=1)\n",
    "\n",
    "# Reattach the SalePrice column to train_data_encoded\n",
    "train_data_encoded['SalePrice'] = sale_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning the encoded DataFrame back to the original variable\n",
    "train_data = train_data_encoded\n",
    "test_data = test_data_encoded\n",
    "\n",
    "# Display the first few rows of the training data after encoding\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Analysis & Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_stats = train_data.describe()\n",
    "print(\"Descriptive Statistics:\\n\", desc_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram for SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    fig = px.histogram(train_data, x='SalePrice', title='Distribution of SalePrice')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the distribution of SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Box plot for SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    fig = px.box(train_data, y='SalePrice', title='Boxplot of SalePrice')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GrLivArea: Above grade (ground) living area square feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    fig = px.scatter(train_data, x='GrLivArea', y='SalePrice', title='GrLivArea vs SalePrice')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    fig = px.scatter(train_data, x='GrLivArea', y='SalePrice', marginal_x='histogram', marginal_y='histogram', title='Joint Plot of GrLivArea vs SalePrice')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalBsmtSF: Total square feet of basement area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    fig = px.scatter(train_data, x='TotalBsmtSF', y='SalePrice', title='TotalBsmtSF vs SalePrice')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OverallQual: Rates the overall material and finish of the house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SHOW_GRAPHS:\n",
    "    overall_qual_mean = train_data.groupby('OverallQual')['SalePrice'].mean()\n",
    "    fig = px.bar(overall_qual_mean, x=overall_qual_mean.index, y='SalePrice', title='OverallQual vs SalePrice')\n",
    "    fig.add_trace(go.Scatter(x=overall_qual_mean.index, y=overall_qual_mean.values, mode='lines', name='lines'))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the correlation of each feature with SalePrice and sort them to identify the strongest relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = train_data.corr()\n",
    "# use abs with the correlation matrix\n",
    "correlation_matrix = correlation_matrix.abs()\n",
    "correlation_with_target = correlation_matrix['SalePrice'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change N to the number of top features you want to analyze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "top_features = correlation_with_target.index[1:N+1] \n",
    "filtered_data = train_data[top_features.to_list() + ['SalePrice']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top correlation with SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Top {N} features with the highest correlation with SalePrice:\")\n",
    "print(correlation_with_target.head(N + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive Statistics with the N Top correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_stats = filtered_data.describe()\n",
    "print(\"Descriptive Statistics for Top Features:\\n\", desc_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairplot Visualizing Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_and_calculate_statistics(filtered_data):\n",
    "    \"\"\"\n",
    "    For each feature in the filtered_data (excluding 'SalePrice'), plots the feature against 'SalePrice'\n",
    "    and calculates relevant statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    - filtered_data: DataFrame containing the top N features most correlated with 'SalePrice' and 'SalePrice' itself.\n",
    "    \"\"\"\n",
    "    # Exclude 'SalePrice' from the features to plot\n",
    "    features = [column for column in filtered_data.columns if column != 'SalePrice']\n",
    "    \n",
    "    for feature in features:\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        if filtered_data[feature].dtype == 'object' or len(filtered_data[feature].unique()) <= 20:\n",
    "            # For categorical features or numerical features with few unique values, use boxplot\n",
    "            sns.boxplot(x=feature, y='SalePrice', data=filtered_data)\n",
    "        else:\n",
    "            # For continuous numerical features, use scatterplot\n",
    "            sns.scatterplot(x=feature, y='SalePrice', data=filtered_data)\n",
    "        plt.title(f'Sale Price by {feature}')\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculating and displaying statistics\n",
    "        if filtered_data[feature].dtype == 'object' or len(filtered_data[feature].unique()) <= 20:\n",
    "            # For categorical features, display mean SalePrice for each category\n",
    "            mean_prices = filtered_data.groupby(feature)['SalePrice'].mean().sort_values(ascending=False)\n",
    "            print(f\"Mean Sale Price for each {feature}:\")\n",
    "            print(mean_prices)\n",
    "        else:\n",
    "            # For continuous numerical features, consider displaying correlation or other relevant statistics\n",
    "            correlation = filtered_data[[feature, 'SalePrice']].corr().iloc[0, 1]\n",
    "            print(f\"Correlation between {feature} and Sale Price: {correlation:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    plot_and_calculate_statistics(filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap for correlation matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def show_top_correlated_features(train_data, n):\n",
    "    \"\"\"\n",
    "    Displays an interactive heatmap for the top n features most correlated with 'SalePrice', including 'SalePrice' at the most right, with annotations for each cell, using Plotly.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_data: DataFrame containing the training data.\n",
    "    - n: The number of top features to consider for the heatmap.\n",
    "    \"\"\"\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = train_data.corr()\n",
    "    \n",
    "    # Find the top n features most correlated with 'SalePrice', excluding 'SalePrice' itself\n",
    "    top_n_features = correlation_matrix['SalePrice'].abs().sort_values(ascending=False)[1:n+1].index\n",
    "    \n",
    "    # Ensure 'SalePrice' is at the end of the list of features to display\n",
    "    features_to_display = list(top_n_features) + ['SalePrice'] \n",
    "    \n",
    "    # Filter the correlation matrix to include only the selected features\n",
    "    filtered_corr_matrix = train_data[features_to_display].corr()\n",
    "    \n",
    "    # Plotting\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=filtered_corr_matrix.values,\n",
    "        x=filtered_corr_matrix.columns,\n",
    "        y=filtered_corr_matrix.index,\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title='Correlation'),\n",
    "        text=[[\"{:.2f}\".format(val) for val in row] for row in filtered_corr_matrix.values],  # Annotations\n",
    "        texttemplate=\"%{text}\",\n",
    "        hoverinfo=\"none\"  # Disable hover info to only show the annotations\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Top {n} Features Correlated with SalePrice (Including SalePrice)',\n",
    "        xaxis_title=\"Features\",\n",
    "        yaxis_title=\"Features\",\n",
    "        xaxis=dict(tickmode=\"array\", tickvals=list(range(len(features_to_display))), ticktext=features_to_display),\n",
    "        yaxis=dict(tickmode=\"array\", tickvals=list(range(len(features_to_display))), ticktext=features_to_display)\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    show_top_correlated_features(filtered_data, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Square Footage\n",
    "- We create a new feature TotalSF by summing up the total basement square footage, first floor square footage, second floor square footage, and garage area. This feature represents the total square footage of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_engineering_list = []\n",
    "\n",
    "# create TotalSF feature\n",
    "def create_TotalSF_feature(data, features_engineering_list=None):\n",
    "    data['TotalSqureF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF'] + data['GarageArea']\n",
    "    if features_engineering_list is not None:\n",
    "        features_engineering_list.append('TotalSqureF')\n",
    "    return data\n",
    "\n",
    "# Create the TotalSF feature for the train and test data\n",
    "train_data = create_TotalSF_feature(train_data, features_engineering_list)\n",
    "\n",
    "test_data = create_TotalSF_feature(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age of the House\n",
    "- We calculate the age of the house at the time of sale by subtracting the year the house was built from the year it was sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create age_of_house feature\n",
    "def create_age_of_house_feature(data, features_engineering_list=None):\n",
    "    data['AgeOfHouse'] = data['YrSold'] - data['YearBuilt']\n",
    "    if features_engineering_list is not None:\n",
    "        features_engineering_list.append('AgeOfHouse')\n",
    "    return data\n",
    "\n",
    "\n",
    "# Create the AgeOfHouse feature for the train and test data\n",
    "train_data = create_age_of_house_feature(train_data, features_engineering_list)\n",
    "\n",
    "test_data = create_age_of_house_feature(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age of the Renovation\n",
    "- We calculate the age of the house since its most recent renovation by subtracting the year of the most recent renovation from the year it was sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create age_of_renovation feature\n",
    "def create_age_of_renovation_feature(data, features_engineering_list=None):\n",
    "    data['AgeOfRenovation'] = data['YrSold'] - data['YearRemodAdd']\n",
    "    if features_engineering_list is not None:\n",
    "        features_engineering_list.append('AgeOfRenovation')\n",
    "    return data\n",
    "\n",
    "# Create the AgeOfRenovation feature for the train and test data\n",
    "train_data = create_age_of_renovation_feature(train_data, features_engineering_list)\n",
    "\n",
    "test_data = create_age_of_renovation_feature(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Bathrooms\n",
    "- We create a new feature TotalBath by summing up the number of full and half bathrooms in the basement and above grade, with half bathrooms counted as 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TotalBath feature\n",
    "def create_TotalBath_feature(data, features_engineering_list=None):\n",
    "    data['TotalBath'] = data['FullBath'] + 0.5 * data['HalfBath'] + data['BsmtFullBath'] + 0.5 * data['BsmtHalfBath']\n",
    "    if features_engineering_list is not None:\n",
    "        features_engineering_list.append('TotalBath')\n",
    "    return data\n",
    "\n",
    "# Create the TotalBath feature for the train and test data\n",
    "train_data = create_TotalBath_feature(train_data, features_engineering_list)\n",
    "\n",
    "test_data = create_TotalBath_feature(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Porch Area\n",
    "- We create a new feature TotalPorchSF by summing up the area of all porch-related features, representing the total porch area of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TotalPorchSF feature\n",
    "def create_TotalPorchSF_feature(data, features_engineering_list=None):\n",
    "    data['TotalPorchSF'] = data['OpenPorchSF'] + data['EnclosedPorch'] + data['3SsnPorch'] + data['ScreenPorch']\n",
    "    if features_engineering_list is not None:\n",
    "        features_engineering_list.append('TotalPorchSF')\n",
    "    return data\n",
    "\n",
    "# Create the TotalPorchSF feature for the train and test data\n",
    "train_data = create_TotalPorchSF_feature(train_data, features_engineering_list)\n",
    "\n",
    "test_data = create_TotalPorchSF_feature(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[features_engineering_list].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_GRAPHS:\n",
    "    show_top_correlated_features(train_data, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Regression Models & Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# split the data into features and target variable\n",
    "X = train_data.drop(['SalePrice'], axis=1)\n",
    "y = train_data['SalePrice']\n",
    "\n",
    "# Encode categorical variables\n",
    "X = pd.get_dummies(X)\n",
    "X_test = pd.get_dummies(test_data)\n",
    "\n",
    "# Align the columns in the test set to match the train set\n",
    "X, X_test = X.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standardize the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function for computing RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_cv(model):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X_train_scaled, y_train, scoring=\"neg_mean_squared_error\", cv=5))\n",
    "    return rmse.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implementing different regression models<br> evaluating them using Cross Validation, and computing the RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg_rmse = rmse_cv(lin_reg)\n",
    "print(f\"Linear Regression RMSE: {lin_reg_rmse}\")\n",
    "\n",
    "# Fit the model and predict\n",
    "lin_reg.fit(X_train_scaled, y_train)\n",
    "y_pred = lin_reg.predict(X_val_scaled)\n",
    "lin_reg_val_rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f\"Linear Regression Validation RMSE: {lin_reg_val_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SGD Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD Regressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_reg_rmse = rmse_cv(sgd_reg)\n",
    "print(f\"SGD Regressor RMSE: {sgd_reg_rmse}\")\n",
    "\n",
    "# Fit the model and predict\n",
    "sgd_reg.fit(X_train_scaled, y_train)\n",
    "y_pred = sgd_reg.predict(X_val_scaled)\n",
    "sgd_reg_val_rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f\"SGD Regressor Validation RMSE: {sgd_reg_val_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "ridge_reg = Ridge(alpha=1.0)\n",
    "ridge_reg_rmse = rmse_cv(ridge_reg)\n",
    "print(f\"Ridge Regression RMSE: {ridge_reg_rmse}\")\n",
    "\n",
    "# Fit the model and predict\n",
    "ridge_reg.fit(X_train_scaled, y_train)\n",
    "y_pred = ridge_reg.predict(X_val_scaled)\n",
    "ridge_reg_val_rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f\"Ridge Regression Validation RMSE: {ridge_reg_val_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg_rmse = rmse_cv(lasso_reg)\n",
    "print(f\"Lasso Regression RMSE: {lasso_reg_rmse}\")\n",
    "\n",
    "# Fit the model and predict\n",
    "lasso_reg.fit(X_train_scaled, y_train)\n",
    "y_pred = lasso_reg.predict(X_val_scaled)\n",
    "lasso_reg_val_rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f\"Lasso Regression Validation RMSE: {lasso_reg_val_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net Regression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net_rmse = rmse_cv(elastic_net)\n",
    "print(f\"Elastic Net Regression RMSE: {elastic_net_rmse}\")\n",
    "\n",
    "# Fit the model and predict\n",
    "elastic_net.fit(X_train_scaled, y_train)\n",
    "y_pred = elastic_net.predict(X_val_scaled)\n",
    "elastic_net_val_rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f\"Elastic Net Regression Validation RMSE: {elastic_net_val_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix the jump!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "house-prices_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
